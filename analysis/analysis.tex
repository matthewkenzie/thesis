\chapter{Analysis}
\label{chap:analysis}
%\chapterquote{Data, data, data. I cannot make bricks without clay}
%{The Adventures of Sherlock Holmes.}

The analysis culminates in the search for a \SM-like Higgs boson decaying into two photons in the mass range $110\leq\mH\leq 150$~\GeV. This is done by making use of two variables which have very different shapes in signal and background. The first is the invariant mass of the two photons, \mgg, relative to the expected signal position, \mH, for which the signal is peaking and the background is smoothly falling. The second is the output of the diphoton \BDT discussed in Sec.~\ref{sec:diphoton_bdt} in which the signal peaks towards positive values and the background peaks towards negative values. The distribution of the invariant mass is shown for data, background and signal \MC simulation in Fig.~\ref{fig:inv_mass_plots} for all events which pass the \MFM analysis cuts described in the previous chapter. The distribution of the diphoton \BDT output is shown in the previous chapter in Fig.~\ref{fig:dipho_bdt}. In the \SMVA analysis these two variables are combined and used to extract a number of analysis bins (see Sec.~\ref{sec:inclusive_cats_sideband}) in which events are counted. In the \MFM events are coarsely categorised according to the diphoton \BDT output and then parametrically fitted as a function of the invariant mass \mgg. The methods used for extracting the background and signal expectations for both analyses are described in this chapter. The important characteristics which define the signal are its yield relative to the \SM expectation, $\mu=\musm$, and its position, \mH.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis/plots/mgg_bkg_7TeV.pdf}
  \includegraphics[width=0.48\textwidth]{analysis/plots/mgg_bkg_8TeV.pdf}
  \caption[Diphoton invariant mass distributions for the datasets at 7 and 8~TeV]{The diphoton invariant mass distribution for the 7 (left) and 8~TeV (right) datasets for the events which pass all of the analysis cuts. Cross-section weighted \MC events are plotted as the filled histograms for the prompt-prompt (green), prompt-fake (yellow) and fake-fake (red) backgrounds. The \SM signal expectation scaled up by a factor of 5 is shown by the blue histogram. The grey bands show the statistical uncertainty (sum of weights) of the background \MC events.}
  \label{fig:inv_mass_plots}
\end{figure}

% ---- SECTION ----
\section{Signal modelling}
\label{sec:signal_model}

The signal \MC samples (described in Sec.~\ref{sec:mc}) are propagated through the full analysis separately for each production mechanism (\ggH, \VBF, \WH, \ZH, \ttH). Events in the samples are weighted by the relevant \SM cross section, branching ratio, the integrated luminosity and the ``detector" weight which compensates for mismodelling in the \MC such as pileup, the beamspot width and discrepancies between the efficiency in data and \MC as measured in \Zee and \Zmumugamma decays. In this way the efficiency and acceptance of the detector, selection and categorisation is mapped by the \MC samples and the total number of expected \SM signal events is given by the sum of weights of the sample. The \MC is generated separately for hypothesised Higgs masses, \mH, in the search range of $110 \leq m_{H} \leq 150$~\GeV in steps of 5~\GeV. For any intermediate points the signal model is interpolated. The efficiency times acceptance, $\epsilon\times\alpha$, of the analysis selection for a \SM Higgs boson is shown in Fig.~\ref{fig:effacc} as a function of the Higgs boson mass, \mH. For a Higgs at $\mH=125$~\GeV $\ea=48.6\pm 0.7\%$ ($49.4\pm 0.7\%$) for the 7 (8)~\TeV datasets.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{analysis/plots/effAcc_vs_mass_7TeV.pdf}
  \includegraphics[width=0.49\textwidth]{analysis/plots/effAcc_vs_mass_8TeV.pdf}
  \caption[The efficiency$\times$acceptance of the analysis selection for \acs{SM} Higgs \acs{MC}]{The efficiency$\times$acceptance of the analysis selection for a \SM Higgs signal as a function of Higgs mass for the 7~\TeV (left) and 8~\TeV datasets.}
  \label{fig:effacc}
\end{figure}

\subsection{Mass-factorised analysis}
\label{sec:signal_mfm}

The diphoton invariant mass signal shape is modelled in a fully parametric way for the \MFM. A separate model, consisting of a sum of Gaussians, is constructed for each production mechanism and each event class, where the two cases of right vertex and wrong (misidentified) vertex are fitted separately. The number of Gaussians used in the sum varies and can be as many as five although usually two or three provides an accurate description of the signal shape. A Gaussian sum is used because it has been found that this most accurately represents the shape of the invariant mass in signal. Many of the event classes contain a mixture of events from the resolution phase space (i.e.~there are mixtures of barrel/endcap and converted/unconverted photons) so by fitting with a sum of Gaussians these different components, which are in themselves Gaussian or close to Gaussian distributed, are accounted for. Some of the event classes for particular production mechanisms contain rather low \MC statistics (for example the lepton tagged category for signal produced by gluon fusion) and so fewer Gaussians are used in these cases. Clearly one expects a low signal yield for these particular cases and so a very accurate description of the shape is unnecessary. By allowing all the parameters in the Gaussian sums to float, including the means of the different Gaussian components, one obtains a parametrisation that models both the peak and tails of the signal distribution accurately. These fits are carried out at each value of \mH where signal \MC is available and then the individual fit parameters are linearly interpolated such that the shape is defined for any \mH in the range $[110,150]$~\GeV. The model therefore includes a floating parameter for the mass of the Higgs which can be fit to data. The right and wrong vertex shapes are then summed, with the relative fraction of right/wrong vertex events, to give a unique signal shape model in each event class for each production process. The signal normalisation, i.e.~the expected number of \SM signal events, is obtained by quadratically interpolating the efficiency $\times$ acceptance for each event class and production process as calculated at the \mH points for which there is \MC. The total signal shape, summed over event classes and production mechanisms, is shown in Fig.~\ref{fig:sig_shape}. The total number of expected signal events, as well as the \sigeff (minimum interval containing 68.3\% of the distribution) and the \sigFW (full width at half the maximum divided by 2.35), for each event class are shown in Table~\ref{tab:sig_shape}. This information is additionally represented diagrammatically in Fig.~\ref{fig:signal_composition} which provides an insightful summary of the analysis. The left hand column of this figure (Fig.~\ref{fig:signal_composition}) shows the breakdown of each signal process in each of the analysis categories. It can be seen that the \textit{untagged} categories contain mostly gluon fusion (green band) where the contribution of the other signal processes decreases when moving from ``Untagged 0" to ``Untagged 4". This is as we expect given that the signal processes which aren't gluon fusion produce Higgs bosons at higher \pT and thus get a higher diphoton \BDT score (see Fig.~\ref{fig:dipho_bdt}). A similar pattern can be seen for the dijet-tagged categories for which the signal consists of predominantly \VBF (red) but increasing amounts of \ggH (green) when moving from ``Dijet 0" - ``Dijet 2". Similarly the \VH-tagged categories consist of mainly \WH and \ZH signal (turquoise and blue) and the \ttH-tagged categories consists of mainly \ttH signal (orange). The middle column of this figure shows the signal model width (in terms of \sigeff and \sigFW) and demonstrates that categories which relate to a high score in the diphoton \BDT (``Untagged 0"/``Untagged 1") and a high score in the combined dijet-diphoton \BDT (``Dijet 0") are those with the best mass resolution. The right hand column of this figure shows the S/(S+B) ratio under the peak (within $\pm$\sigeff of \mH) and demonstrates the sensitivity of the individual categories.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.49\textwidth]{analysis/plots/ThesisFits/mva_7TeV/all.pdf}
    \includegraphics[width=0.49\textwidth]{analysis/plots/ThesisFits/mva_8TeV/all.pdf}
    \caption[The diphoton invariant mass shape for \SM Higgs signal]{The diphoton invariant mass shape of the \SM signal for the 7~\TeV dataset (left) and 8~\TeV dataset (right). The black square points show the distribution in \MC events, where the sum of weights represents the expected number of signal events from a \SM Higgs boson at 125~\GeV. The blue line shows the shape of the parametric model used to represent the signal.}
    \label{fig:sig_shape}
  \end{center}
\end{figure}

\input{analysis/plots/EventClassFractions}
  
\begin{figure}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{analysis/plots/ThesisFits/mva_7TeV/signalComposition_fix.pdf} \\
    \includegraphics[width=0.9\textwidth]{analysis/plots/ThesisFits/mva_8TeV/signalComposition_fix.pdf}
    \caption[The composition and width of the signal in each analysis category]{The expected composition and resolution of the signal for a \SM Higgs at 125~\GeV in the 7~\TeV dataset (top) and 8~\TeV dataset (bottom). The left diagram shows the breakdown of the signal in each event class by category: \ggH (green), \VBF (red), \WH (turquoise), \ZH (blue), \ttH (orange). The middle diagram shows the expected resolution in each event class in terms of \sigeff and \sigFW. The right diagram shows the expected signal over signal-plus-background ratio in a window of $\pm\sigeff$ around \mH=125~\GeV.}
    \label{fig:signal_composition}
  \end{center}
\end{figure}
  
\subsection{Sideband analysis}
\label{sec:sig_sideband}

The statistical method used for the sideband analysis is not a parametric shape analysis like the \MFM but implemented as a simple counting experiment, across the analysis categories, inside the signal region, which is defined as a $\pm2\%$ window around the hypothesised Higgs mass, \mH. The signal invariant mass shape is included by the categorisation scheme described in Sec.~\ref{sec:inclusive_cats_sideband} whereby events nearer the signal peak are collected in bins with higher sensitivity (see Fig.~\ref{fig:sideband_cats}). The number of expected signal events in each category is simply obtained from the sum of weights of the \MC samples in each category which fall inside window for values of \mH for which there exists \MC samples. The signal shape, i.e.~the signal distribution across the analysis categories, is linearly interpolated for any intermediate values of \mH. As for the \MFM the \SM production mechanisms are propagated through the analysis separately. The signal normalisation at each Higgs mass is calculated in a similar way as for the \MFM in which the \ea is linearly interpolated between the masses at which there are \MC samples and then scaled by cross section, branching ratio and integrated luminosity.

\subsection{Spin analysis}

In the spin analysis the signal models are obtained from \MC simulation as described in Sec.~\ref{sec:mc} for the spin-0 \SM processes and the spin-2 processes. A parametric model identical to the one built for the nominal analysis is constructed as per the method described in Sec.~\ref{sec:signal_mfm}. The acceptance $\times$ efficiency of the two spin models in each category as well as the differential cross section as a function of \abscostheta, which depends only on the spin of the initial state, are obtained from the \MC simulation. The only remaining assumption is on the total number of expected signal events for a given spin-parity state and production mode. This is well defined for the spin-0 SM case and is obtained from the $\sigma\times BR$ given by the LHC Higgs cross section working group in~\cite{LHCHiggsCrossSectionWorkingGroup3}. For the graviton-like \twomp this quantity is unknown. 
%Consequently, when generating pseduo-experiments for a particular model, the model is first fitted to the data to extract the relative shapes and normalisations of the signal and background. 
Consequently we scale the signal models for both spin hypotheses with a modifier, $\mu$, such that when $\mu=1$ the total number of expected signal events for the model in question is equivalent to the SM expectation. 

% ---- SECTION ----
\section{Background modelling}
\label{sec:background_model}

The background is the most significant unknown in this analysis. The size of the background relative to the signal is large and the invariant mass shape of the background is poorly modelled in \MC. It is known to be a smoothly varying and falling spectrum, however various detector effects such as reconstruction, energy resolution and triggering which are imperfectly modelled in the \MC can distort the shape. Furthermore the contribution of fake photons to the background varies as a function of \mgg and this is not well modelled in the theory or detector simulation. One of the main motivations for having the two analyses described here is that they have completely different ways of measuring the background and consequently serve to cross-check each other. Both analyses use entirely data-driven methods for extracting the background.  

\subsection{Mass-factorised analysis}
\label{sec:envelope}

A unique and novel way of estimating the background, given we have no \emph{a priori} knowledge of its shape, has been developed specifically for this analysis. The method, latterly referred to as the ``envelope" method, attempts to parametrise our ignorance of the background shape in a similar way to what is done for normal nuisance parameters when using the negative log likelihood to obtain the best fit value of a quantity and its error using frequentist statistical methods~\cite{FredJames}. To explain better the method let us first consider the simplified case of fitting a probability distribution to a dataset where the probability distribution contains one physical \POI, $x$, (which could for example represent the signal mass) and one nuisance parameter, $\theta$, (which could represent the energy scale uncertainty). In this case we can calculate twice the negative log likelihood, $-2$LL, at different values of $x$ whilst at each point minimising the likelihood with respect to \theta. 
This would give us the best fit value of $x$ and its error given the variation allowed by \theta. This is represented in Fig.~\ref{fig:envelope_explain1} by the solid black line. The best fit value of $x$ is given at the minimum of the likelihood and the 1$\sigma$ error interval is defined as the range where twice the negative log likelihood relative to the global minimum (best fit value) is less than one, \NLL$\leq1$. This is known as the ``profile likelihood method" as the nuisance parameter, $\theta$, is ``profiled" (i.e.\ floated) in the fit. In other words, for a given value of $x$ the value of $\theta$ which minimizes the likelihood is chosen. One can redo the likelihood scan fixing $\theta$ to its value at the best fit and this gives a narrower likelihood curve, demonstrated by the dashed blue line in Fig.~\ref{fig:envelope_explain1}. In this case the error, the range for which \NLL$\leq1$, is smaller as one would expect given the nuisance parameter is now frozen. This is equivalent to the statistical error only, as the systematic component parametrised by $\theta$ is ignored. One can now also build up a multitude of curves by setting the nuisance parameter $\theta$ to arbitrary values and rescanning the likelihood, these are shown in Fig.~\ref{fig:envelope_explain1} as the red lines. It should be clear that by taking the ``envelope" around the potentially infinite set of red curves one can reproduce the black line representing the full profile fit, providing the full $\theta$ phase space is sampled enough times. 
This is shown in the figure by the green line which becomes smoother as more sets of $\theta$ values are chosen. It is worth noting that not all of the red curves (of which there are infinitely many) necessarily have to touch the black line: a very extreme value of $\theta$ will give a bad fit and the corresponding red dashed curve would be off the plot. In this way one could in principle ``reverse-engineer" the profile likelihood method such that the profiling of the nuisance parameter $\theta$ is not done as a continuous minimisation but as a series of discrete minimisations, where the minimum \NLL for a given value of $x$ is taken as the minimum \NLL over all discrete choices of $\theta$ at this value of $x$. Clearly, for a case in which the nuisance parameter $\theta$ is continuous this is inefficient and unnecessary but it is effective if the nuisance parameter can only take discrete values.

\begin{figure}
\begin{center}
  \includegraphics[width=0.8\textwidth]{analysis/plots/envelope_explain.pdf}
  \caption[Conceptual idea of the method of likelihood profiling]{Conceptual idea of the method of ``profiling the likelihood". Here the \NLL is scanned as a function of a physical parameter of interest $x$ with a nuisance parameter $\theta$ in three cases: 1) where $\theta$ is freely floating (black line), 2) where $\theta$ is set to its value at the \NLL minimum (blue line), 3) for several arbitrary choices of $\theta$ (red line). The green line shows the ``envelope" around the total minimum over several discrete choices of $\theta$. After an infinite sampling of $\theta$ the green line would match the black.}
  \label{fig:envelope_explain1}
\end{center}
\end{figure}

In this analysis the background parametrisation is entirely unknown. So in principle if we could sample the infinite phase space of possible function choices we could use the method just described to profile the choice. In this way we find the function that minimises the negative likelihood for any given value of the parameter of interest $x$. This will then pick the function which fits the data best (minimises the negative likelihood) but can enlarge the error given that many different functions are tried.
Let's now consider a simplified case of the real situation. Imagine we have a large steeply falling background which can be parametrised by two possible choices: a single term power law, $x^{-p}$, and a single term exponential, $e^{-px}$. We have a small Gaussian-like signal component and our \POI is the size of this signal, $\mu$. The best fit distributions are shown for a generated pseudo-dataset in the left hand plot of Fig.~\ref{fig:envelope_explain2}. The right hand plot of Fig.~\ref{fig:envelope_explain2} shows the likelihood scan across the parameter $\mu$, which represents the size of the signal, for the two chosen functions (blue and red lines). The envelope (the minimum of the negative likelihood across both function choices) is shown as the yellow dashed line. One can see that the global best fit from the envelope is at a value of $\mu=2.83$ which is that obtained with the power law function. The $1\sigma$ error (the point where the \NLL crosses 1) is unchanged in the envelope with respect to the result using the power law function alone. However, the $2\sigma$ error (the point where the \NLL crosses 4) is increased with respect to the result using the power law function alone. This is the principle behind the ``envelope" background method. One can see that it is analogous to using a normal nuisance parameter which can only take discrete values. In this case the discrete values index which function is chosen. It means that a specific function choice never has to be made and the error on a given value will increase to account for situations where two or more functions give a similarly good fit. However, there is one more important feature of the method which must be discussed before it can be applied to the data.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.49\textwidth]{analysis/plots/envelope_explain2.pdf}
    \includegraphics[width=0.49\textwidth]{analysis/plots/envelope_explain3.pdf}
    \caption[A toy example of using the envelope method for estimating the background]{An example of using the envelope in a realistic situation. The plot on the left shows a signal plus background fit to some pseudo-data using two different background function choices: a single term power law (red) and a single term exponential (blue). The plot on the right shows the profile likelihood curve for the signal size, $\mu$, for the two different function choices. The global minimum is at $\mu=2.83$ and the power law is used as the background choice. The envelope likelihood curve, profiling over the function choices is shown as the yellow dashed line.}
    \label{fig:envelope_explain2}
  \end{center}
\end{figure}

The concept of the ``envelope" method has now been demonstrated with a simplified example in which two function choices were profiled and both these functions had one free parameter. However we would like to be able to sample as much of the function phase space possible (i.e.~as many functions as we can) which means there will be choices with different numbers of free parameters. Two functions of the same type but of a different order may give a very similar looking fit but functions with more free parameters are more flexible and thus will give a lower negative likelihood value. For example consider the functions $E_{1} = e^{-p_{1}x}$ and $E_{2} = f_{1}e^{-p_{1}x}+(1-f_{1})e^{-p_{2}x}$, which are first and second order exponential sums and have one and three free parameters respectively, not including an overall normalisation term. There may be a case where these give an identical fit (as $f_{1}\to 1$ or $p_{1}\to p_{2}$) however the negative log likelihood will always be lower for a higher parameter fit and therefore the higher order function would always be the minimum of the envelope. This means that for any embedded class of functions, the function which gives the global minimum of the likelihood will be of the highest order allowed. Consequently, a correction scheme has been devised to avoid this problem and to penalise functions in the envelope which have more free parameters but don't necessarily fit the data any better.

The negative log likelihood gets redefined as:
\begin{equation}
  -2\mathrm{LL} = -2\ln(\mathcal{L}) + cN_{p},
\end{equation}
where $\mathcal{L}$ is the likelihood for the function, $N_{p}$ is the number of free parameters in that function and $c$ is a constant correction term. Two correction schemes were studied, for values of $c=1$ and $c=2$. The motivation for choosing $c=2$ is an application of the Akaike information criterion, described in Ref.~\cite{akaike}, which states that for large sample sizes the corrected likelihood is
\begin{equation}
  A = -2\ln(\mathcal{L}) + 2N_{p}.
\end{equation}
It can be seen that the correction term here is $2N_{p}$, which gives a value of $c=2$, in others words a correction of 2 per free parameter. 

The argument for using a correction of $c=1$ is a little more natural and arises from the assumption that in the high statistics limit for a binned dataset, $\NLL\approx\chi^{2}$. Given a value of the $\chi^{2}$ one can calculate the $\chi^{2}$ $p$-value given the number of degrees of freedom, $N$, which is equal to the number of bins, $b$, minus the number of free parameters in the fit function, $N_{p}$. Thus the $p$-value can be expressed as $p(\chi^{2},b-N_{p})$. One can then determine a new chi-squared value, $\chi^{\prime 2}$, defined as the one which would give the equivalent $p$-value but with a different number of degrees of freedom, namely the one in which there are no free parameters in the fit, $p(\chi^{\prime 2},b)$ . It can be seen that there is now an expression for $\chi^{\prime 2}$, which is approximately equivalent to a new \NLL (now relative to the best possible fit, given the data), which is independent of the number of fit parameters, $N_{p}$, and thus the corrected likelihood is given by,
\begin{equation}
  -2\mathrm{LL} = -2\ln(\mathcal{L}) + \chi^{\prime 2} - \chi^{2}.
\end{equation}
The correction term, $\chi^{\prime 2} -\chi^{2}$, depends on the number of bins, the number of free fit parameters and the quality of the original fit (the $\chi^{2}$ $p$-value). Figure~\ref{fig:envelope_chi2_correction} shows how the size of the correction varies for functions with different numbers of fit parameters. It can be seen that on average the correction is given by,
\begin{equation}
  \chi^{\prime 2} - \chi^{2} \approx N_{p}.
\end{equation}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.49\textwidth]{analysis/plots/ChisqConv1.png}
    \includegraphics[width=0.49\textwidth]{analysis/plots/ChisqConv2.png}
    \caption[An estimation of the correction required in the envelope method]{The value of the correction, $\chi^{\prime 2} - \chi^{2}$, as a function of the fit $p$-value for a fit with 320 bins is shown on the left. The projection of the correction integrated uniformly over $p$-values is shown on the right. The five different coloured lines represented fit functions with different numbers of free parameters, ranging from one free parameter (pink) up to five free parameters (black).}
    \label{fig:envelope_chi2_correction}
  \end{center}
\end{figure}
The correction scheme used in the analysis was chosen to be $c=1$. This was decided using empirical results of studying the impact on the bias and error coverage when fitting the physical parameters of the signal yield, $\mu$, and the signal position, \mH, using the envelope method with either correction scheme. Before we cover the results and conclusions of this study, we will first discuss how one decides which functions to include in the envelope.

In principle it would be beneficial to choose any and every function one can think of that can reasonably describe a falling spectrum. In practice this is unfeasible as the combinatorics of the problem rapidly spiral out of control. If an analysis has multiple categories, $N_{c}$, and one chooses multiple background functions in each category, $N_{f}$, then the number of combinations goes like $N_{f}^{N_{c}}$ which rapidly makes the problem computationally impossible. Instead, one has to choose a smaller number of functions which reasonably cover the phase space of infinite functions. The functions used in this analysis come in four main classes. They are as follows (note an overall normalisation term is not included in the equations below),

\begin{itemize}
  \item \textit{Exponential recursive sum} - a sum of terms like $e^{-px}$.
    \begin{equation}
      p(x) = c_{1}e^{-p_{1}x} + (1-c_{1})c_{2}e^{-p_{2}x} + (1-c_{2})c_{3}e^{-p_{3}x} + ... + (1-c_{n-1})c_{n}e^{-p_{n}x}
    \end{equation}
    \begin{itemize}
      \item This has $2n-1$ free parameters per order. The functions are labelled by the number of free parameters so the lowest order is \texttt{exp1} then \texttt{exp3}, \texttt{exp5} and so on.
    \end{itemize}

  \item \textit{Power law recursive sum} - a sum of terms like $x^{-p}$.
    \begin{equation}
      p(x) = c_{1}x^{-p_{1}} + (1-c_{1})c_{2}x^{-p_{2}} + (1-c_{2})c_{3}x^{-p_{3}} + ... + (1-c_{n-1})c_{n}x^{-p_{n}}
    \end{equation}
    \begin{itemize}
      \item This has $2n-1$ free parameters per order. The functions are labelled by the number of free parameters so the lowest order is \texttt{pow1} then \texttt{pow3}, \texttt{pow5} and so on.
    \end{itemize}

  \item \textit{Laurent series} - The best fit value for a single order power term is around -4.3. Consequently we use a Laurent-like series of a sum of terms like $x^{-n}$ expanded around $x^{-4}$.
    \begin{equation}
      p(x) = c_{1}x^{-4} + (1-c_{1})c_{2}x^{-5} + (1-c_{2})c_{3}x^{-3} + (1-c_{3})c_{4}x^{-6} + ...
    \end{equation}
    \begin{itemize}
      \item This has $n-1$ free parameters per order. The functions are labelled by the number of free parameters so the lowest order is \texttt{lau1} then \texttt{lau2}, \texttt{lau3} and so on.
    \end{itemize}

  \item \textit{Bernstein polynomials} - These are polynomials in the Bernstein basis~\cite{bernsteins1,bernsteins2}. A Bernstein polynomial of degree $n$ is given by,
  \begin{equation}
    p(x) = \displaystyle\sum_{1}^{n}c_{i}\frac{n!}{i!(n-i)!}x^{i}(1-x)^{n-i}
    \label{eq:bernsteins}
  \end{equation}
  \begin{itemize}
    \item This has $n$ free parameters per order. The functions are labelled by the number of free parameters so the lowest order is \texttt{pol1} then \texttt{pol2}, \texttt{pol3} and so on.
  \end{itemize}
\end{itemize}

One then has to determine which order functions of each of these classes is used in the envelope for a given dataset. To pick the lowest order included in the envelope a simple goodness of fit test is used and has the loose requirement that the $\chi^{2}$ $p$-value of the fit is greater than 0.01. To pick the highest order used in the envelope a Fisher test ($f$-test) requirement is imposed~\cite{fisher}, which can be described as follows. If two functions of the same class have $n$ and $n+m$ free parameters respectively then, in the high statistics limit, the difference in \NLL between the two functions is distributed as $\chi^{2}$ with $m$ degrees of freedom. Consequently the difference in \NLL between the two fits is converted into a $\chi^{2}$ $p$-value and the next higher order function is included if $p<0.1$. In other words if the higher order function does not improve the fit sufficiently it is not included. This determines which functions are included in the envelope for each category and ranges anywhere between four and nine. It should be noted that the requirements to include a function are intentionally loose so that as much of the ``function-space" is sampled as possible.

In order to assess the bias and coverage properties of the envelope method a few functions are chosen as ``truth" models from which to generate pseudo-data and test the statistical validity of the method. A single function of each class is chosen as a ``truth" model and the order determined by the same $f$-test but with a tighter requirement that $p<0.05$. These models are first fit to the data and then the pseudo-data generated from these values. The comparison of the systematic bias to statistical uncertainty is determined by the pull distribution,
\begin{equation}
  pull(\mu) = \frac{\mu_{fit} - \mu_{inj}}{\sigma_{fit}},
\end{equation}
where $\mu_{inj}$ is the injected signal strength in the toy, $\mu_{fit}$ is the fitted signal strength per toy and $\sigma_{fit}$ is the error on the fitted signal strength per toy. The systematic bias to statistical comparison is then the deviation of the mean of the distribution from zero as compared to the width of this distribution. An unbiased method will give a mean close to 0 and a method which covers accurately will give a width equal to 1. The $X\sigma$ coverage is defined as the fraction of toys for which $\mu_{fit}-X\sigma_{fit}\leq\mu_{inj}\leq\mu_{fit}+X\sigma_{fit}$.

When using a correction factor of $c=1$, the systematic bias on the signal strength is less than 14\% of the statistical error from the fit and the coverage is accurate for 0.5, 1, 2 and 3$\sigma$ (all the points tested). This was found to be true when generating toy experiments for a range of different signal strengths and for signal at different mass values. The bias and coverage have also been tested when removing the ``truth" model from the set of envelope functions, when removing all functions of the same class as the ``truth" model and when using various convoluted truth models such as a histogram spline of the data~\cite{regression_spline}, a kernel density estimator (sum of Gaussians)~\cite{kde} and a hybrid of different functions patched together. All of these cases also demonstrated a reasonable level of bias and coverage. 

When using a correction factor of $c=2$ the systematic bias is larger than this. In extreme cases it can reach 30\% of the statistical uncertainty and in these cases it is common that the method undercovers especially at higher standard deviations, $\sigma$. Consequently, for this analysis a correction factor of $c=1$ is chosen. The tests described above to ascertain the statistical validity of the method are performed for each individual category of the analysis and furthermore for the categories combined together in years and the whole ensemble of categories. The method has sensible behaviour in all of these cases.

The invariant mass distributions with the different envelope functions, after having been fit to the data, for each of the analysis categories are shown in Appendix~\ref{sec:app_env}. %Figs.~\ref{fig:multipdf1}-\ref{fig:multipdf4}.

\subsection{Sideband analysis}

It is desirable, for the same reasons as in the \MFM analysis, to extract the background estimation in the \SMVA analysis from data. Given that the \SMVA is simply a cut and count analysis in a signal window, of size $\pm2\%$, around the hypothesised Higgs mass, \mH, a fully parametric description of the background in each category is not required. It is simply a case of estimating the number of expected background events in the signal region for each of the \SMVA categories, which are defined by picking out regions of the two-dimensional phase space, $\Delta m/m_{H}$ vs.~diphoton \BDT output, as shown in Fig.~\ref{fig:sideband_cats} and described previously in Sec.~\ref{sec:inclusive_cats_sideband}. The total expected number of background events and the fraction populating each of the \SMVA categories are extracted separately.

The total number of expected background events, i.e.~the overall normalisation of the background, in the signal region is obtained by fitting the invariant mass distribution, summed over all the categories, in the range $100 \leq \mgg \leq 180$~GeV where the data in the signal region is excluded from the fit. Figure~\ref{fig:sideband_norm} shows the invariant mass distribution for all events in data and the parametric fit used to extrapolate the number of background events in the signal region. The parametric form used to obtain the normalisation of the background is a single term power law (one degree of freedom) for the 8~\TeV dataset and a two term Laurent series (one degree of freedom) for the 7~\TeV dataset. The functions are defined identically to those described for the envelope method in Sec.~\ref{sec:envelope}. These functions are chosen because they give the smallest uncertainty when accounting for biases incurred by picking the wrong functional form. A systematic uncertainty is applied to the normalisation term of which there are more details in Sec.~\ref{sec:systematics}.

The fraction of background events which populate each category is extracted using the data in invariant mass sidebands. The signal region is defined as $\pm2\%$ window around the hypothesised mass, \mH. Each sideband is defined to have the equivalent width of $\pm2\%$ relative to the invariant mass at the centre of the sideband, thus the sidebands on the upper side are wider than the sidebands on the lower side. For a \SM Higgs at 125~\GeV about 75\% of the signal is contained within the window. Consequently, one sideband either side of the signal window is skipped and then three sidebands on either side of the hypothesised mass in question are used. Also shown in Fig.~\ref{fig:sideband_norm} are the signal region and the six sidebands for \mH=125~\GeV. The Higgs masses tested in the sideband analysis are between $115 \leq \mH \leq 150$ and consequently both the signal region and the sidebands slide given the value of \mH under consideration. Because of Drell-Yan contamination (\Zee misidentified as two photons) in the region of low invariant mass, $\mgg<100$~GeV, the sideband analysis only considers signal in the range $115\leq \mH \leq 150$ so that the bottom edge of the lowest sideband at $\mH=115$~GeV is above $100$~GeV. 
\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{analysis/plots/sideband/invmass.pdf}
    \caption[The invariant mass distribution for the 8~\TeV dataset]{The inclusive invariant mass distribution for the 8~TeV dataset in data (black points). An illustration of the signal region (red) and the sidebands (blue) used for the mass hypothesis $\mH=125$~\GeV is shown. The parametric function used to obtain the normalisation in the signal region is shown as the blue line.}
    \label{fig:sideband_norm}
  \end{center}
\end{figure}

The data is then split into categories and a straight line fit is performed in each which uses the fraction of data events, contained in the given category, in each sideband to estimate the fraction of background events, contained in the given category, in the signal region. An example of one of these fits is shown in Fig.~\ref{fig:sideband_shape} for ``Category 0" in the 8~\TeV dataset. These straight line fits are performed for each category simultaneously with the constraint that the sum across all the analysis categories of the fraction of background events estimated in the signal region is equal to one. It is assumed that the fraction of events in each category varies linearly with invariant mass (i.e.~hence the straight line across sidebands) and that there is negligible signal contamination in the sidebands. For each category, $c$, the fraction of events for a given mass is taken to be
\begin{equation}
  f_{c} = p_{0c} + p_{1c}(m-m_{H}),
\end{equation}
where $p_{0c}$ and $p_{1c}$ are the straight line fit parameters for category $c$. Since the fractions must sum to unity for any given mass (the normalisation is extracted elsewhere) then for $N$ bins there are $2(N-1)$ coefficients. These coefficients are determined by the straight line across the sidebands in each category, with the constraint that the fraction in each sideband must sum to one across all the categories. 
\begin{figure}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{analysis/plots/sideband/sideband_fit.pdf}
    \caption[An example of one of the sideband fits for the \acs{SMVA}]{The fraction of data events in a single analysis category in each of the sidebands surrounding the signal window at $\mH=125$~\GeV (black points), shown for the \SMVA ``Category 1" (or ``Bin 1") in the 8~TeV dataset. The blue line shows the straight line fit for this bin. In the 8~\TeV dataset there are 10 inclusive analysis bins and 9 exclusive analysis bins. Consequently there are 19 of these straight line fits peformed simultaneously at each \mH but only 18 of them are independent as the fraction of events obtained in the signal region must sum to unity.}
    \label{fig:sideband_shape}
  \end{center}
\end{figure}
The distribution for the data, background (along with the $\pm1\sigma$ and $\pm2\sigma$ errors) and signal for each of the sideband analysis bins are shown in Fig.~\ref{fig:sideband_output} for the mass hypothesis, \mH=124.7~\GeV.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis/plots/sideband/sideband_model_bfmass_7TeV.pdf}
  \includegraphics[width=0.48\textwidth]{analysis/plots/sideband/sideband_model_bfmass_8TeV.pdf}
  \includegraphics[width=0.48\textwidth]{analysis/plots/sideband/sideband_model_bfmass_7TeV_diff.pdf}
  \includegraphics[width=0.48\textwidth]{analysis/plots/sideband/sideband_model_bfmass_8TeV_diff.pdf}
  \caption[The distribution of data, background and signal for the \acs{SMVA}]{The distribution in sideband analysis categories (bins) for the data (black points), the background (blue line), the error on the background (green and yellow bands) and the signal (red line) for the mass hypothesis at \mH= 124.7~\GeV. The top row shows the expected number of events of each type for the 7~\TeV (left) and 8~\TeV (right) datasets. The bottom row shows the background subtracted plot for data (black points) and signal (red line).}
  \label{fig:sideband_output}
\end{figure}

\subsection{Spin analysis}

Two statistical tests are carried out in the spin analysis:

\begin{enumerate}
  \item The signal strength, $\mu$, is extracted differentially in bins of \abscostheta. This is a relatively model-independent test and in principle allows any spin model to be compared to the data.
  \item The statistical separation between different spin hypotheses is calculated using a test statistic similar to the one described in Sec.~\ref{sec:stats} and the $CL_{s}$ exclusion method is used to quantify the separation power. This is a highly model-dependent test but allows for the exclusion of specific spin models.
\end{enumerate}

The background model for the spin analysis comes in two forms. For the differential measurement of the signal strength in bins of \abscostheta the envelope background method is used as per the description in Sec.~\ref{sec:envelope}. However, when calculating the statistical separation between various spin hypotheses a single parametrisation of the background is used in each category, namely a polynomial in the Bernstein basis~\cite{bernsteins1,bernsteins2} as per the description in Eq.~\ref{eq:bernsteins}. The reason for this is that there is no asymptotic approximation for the test statistic distribution when the null hypothesis is not embedded in the alternative hypothesis. Consequently, in order to obtain the test statistic distributions (like the ones shown in Fig.~\ref{fig:cls}) one has to generate lots of pseudo-data and then refit this data to obtain the likelihood ratio and hence test statistic value. Given the complexity of the spin signal model and the combinatorics involved when using the envelope method with 40 analysis categories this becomes impractical in terms of computational power. It has been trialled using the GRID computing network but was found to take the order of hundreds of CPU years.
Given this complication, and the fact that small losses in sensitivity to the background normalisation have a small impact on the spin hypothesis separation power, a single parametrisation in each category was chosen for ease and simplicity. The choice is to use 4th order Bernstein polynomials in all categories apart from the highest \abscostheta categories in which a 3rd order was chosen. The motivation behind this choice is that these order of polynomials show a similarly small level of bias as the envelope method when tested against ``truth" models for the spin categories analogous to the description in Sec.~\ref{sec:envelope}.


% ---- SECTION ----
\section{Systematic Uncertainties}
\label{sec:systematics}

There are several sources of systematic uncertainty in the analysis and these are described in the following section. Nearly all of these affect the signal model, although there is an uncertainty associated with the ``envelope" background method, discussed in Sec.~\ref{sec:envelope}, for the \MFM and there are two uncertainties to the background for the \SMVA, discussed separately below. There is one additional systematic for the spin analysis, also discussed separately below. The signal systematics come in three broad categories; those associated with individual photons, those associated with individual events and those associated with specific tagged event classes. The systematics get applied to the model in different ways depending on the level of correlation required between them. The \MFM analysis has a more precise correlation model between photon energy uncertainties and the mass shape, predominantly because this is used for the main result and is required to provide a measurement of the observed boson's mass. For the \SMVA this is somewhat simplified as the analysis only serves as a cross-check of the categorisation and background estimation for which the important systematics are on the overall signal yield and not so much its shape or position. The way the systematics are implemented in the two analyses is explained below, after which there is a description of the individual uncertainties used. There is a full summary of the systematics given in Table~\ref{tab:systematics}. The main groups of systematic uncertainty affecting the signal strength, $\sigma/\sigma_{\mathrm{SM}}$, are shown in Table~\ref{tab:systematics_mu} and the main groups of systematic uncertainty affecting the uncertainty on the mass measurement, \mH, are shown in Table~\ref{tab:systematics_mh} at the end of the section.

\subsection{Implementation of the systematic uncertainties}

\subsubsection{Mass Factorised Analysis}

In the \MFM uncertainties are implemented in two different ways. The first type are generally associated with the photon energy scale and resolution and the differences between photons and electrons. These are implemented as nuisance parameters with a Gaussian constraint, and affect the position, shape and normalisation of the signal probability distribution function. The second type are uncertainties which result in events being misclassified or rejected by the event selection. These are implemented as nuisance parameters with a log normal constraint of the following form:
\begin{equation}
  f(x,\mu,\sigma) = \frac{1}{x\sqrt{2\pi}\sigma}e^{-\frac{(\ln x - \mu)^{2}}{2\sigma^{2}}},
\end{equation}
where \mu and \sigma represent the mean and standard deviation of the variable $x$'s natural logarithm,
and affect the normalisation of the signal and the relative signal yield in each event class. Log normal constraints
are chosen for the latter because they represent Gaussian-like constraints but are distored such that the nuisance term cannot drive the yield negative. These uncertainties are somewhat easier to understand as they simply dictate that events can migrate between categories and that the overall normalisation of the signal can vary due to the theoretical cross section, luminosity measurement, etc. The former type is perhaps slightly harder to understand as these uncertainties are applied to individual photons and their effect is propagated through to the invariant mass shape and normalisation of the signal for each production process and event class. The methodology for this is as follows:

\begin{itemize}
  \item Each uncertainty is designed to address a specific class of photons with similar properties. For example one uncertainty might be the energy scale uncertainty for unconverted photons in the barrel. We can label this class $c$ and the uncertainty for this class $\theta_{c}$ which is the nuisance parameter that enters the signal model.
  \item Within the class $c$ there may be several subgroups of photons which have different uncertainties. For example photons in the central barrel and outer barrel. We will label this uncertainty as $\sigma_{s}$.
  \item The full analysis is run through where each photon in class $c$ has its scale altered according to a random Gaussian number centred on its nominal value with a width of its individual uncertainty $\sigma_{s}$ and all photons in the other classes are unchanged. 
  \item The distributions of the diphoton invariant mass for the signal, with and without the above random shift, are compared. The relative difference between the mean, effective width and overall rate of these two distributions is calculated, labelled $k_{\mu}$, $k_{\sigma}$ and $k_{r}$ respectively, for each signal process and analysis event class separately. This then provides a measure of the $1\sigma$ systematic effect on the signal mass shape of the uncertainty $\theta_{c}$.
  \item Given there are several nuisance parameters $\theta$ for which this is done, the signal model Gaussian parameters then get reparametrised as follows;
    \begin{flalign}
      & \mbox{Gaussian mean:} \;\;\;\;\; \mu(\mH) = \hat{\mu}(\mH)\Biggl(1+\displaystyle\sum_{c}k_{\mu c}\theta_{c}\Biggr) & \\
      & \mbox{Gaussian sigma:} \;\;\;\; \sigma(\mH) = \hat{\sigma}(\mH)\Biggl(1+\sqrt{\displaystyle\sum_{c}k_{\sigma c}^{2}\theta_{c}^{2}}\Biggr) & \\
      & \mbox{Signal rate:} \;\;\;\;\;\;\;\;\;\;\; r(\mH) = \sigma_{\mathrm{xs}}\cdot \mathcal{B} \cdot \epsilon\cdot\alpha \cdot L\Biggl(1+\displaystyle\sum_{c}k_{rc}\theta_{c}\Biggr) &
    \end{flalign}
where $\hat{\mu}$ and $\hat{\sigma}$ represent the nominal fitted values of the Gaussian means and widths in the \MC and $\sigma_{\mathrm{xs}}$, $\mathcal{B}$ and $L$ represent the cross-section, branching ratio and luminosity repsectively.
\end{itemize}
The final event classes contain mixtures of photons from several different regions (in \eta and \rnine) which all have different associated uncertainties. Using this method the full correlation between the individual photon uncertainties is completely mapped onto the final invariant mass signal shape of each analysis category. Each shape parameter of the signal model is given a dependence on each of the nuisance parameters. The signal shape can then be altered according to the size of the effect the nuisance parameter has in that region of signal phase space.

\subsubsection{Sideband analysis}

As the sideband analysis is simply a cut-and-count experiment across 15 (19) bins in the 7 (8)~\TeV datasets the uncertainties are just implemented as normalisation errors affecting the yield in each bin. These can arise from the energy scale, resolution, reconstruction efficiency, etc., and result in event migrations between bins or arise from the theoretical cross section, luminosity measurement, etc., and affect the overall signal normalisation. There are two systematics in the \SMVA associated to the background estimation from the sidebands. The first arises from the assumption that the background varies linearly across sidebands, although this has a negligible impact on the overall signal yield error. The second arises from the assumed shape used to extract the background normalisation. This is the dominant systematic in the analysis and is extracted using a bias study similar to the one described for the envelope method in Sec.~\ref{sec:envelope}. The size of this uncertainty is 0.7\% (0.4\%) on the background rate in the 7 (8)~\TeV dataset.

\subsubsection{Spin analysis}

One additional uncertainty is used in the spin analysis to account for mismodelling of the \pT spectrum of the signal in \MC events. There is a considerable correlation between the graviton \pT and the decay angle, \costhetastar, and this distribution is not well known theoretically. Consequently, the graviton \pT spectrum is reweighted to match the \SM prediction and an uncertainty is applied to allow events to migrate between the \abscostheta categories given a 10\% change in the graviton \pT.

\subsection{Systematic uncertainties related to individual photons}

\subsubsection{Photon energy scale}
In Section~\ref{sec:photon_energy} a method for correcting the photon energy using \Zee decays was described. Although the statistical uncertainty on these corrections is small, there are some data/\MC discrepancies that gives rise to a systematic uncertainty. This uncertainty is individually calculated for the 8 categories (4 in \eta $\times$ 2 in \rnine) for which the energy scale correction is applied. There are four nuisance parameters for each of the 7 and 8~\TeV datasets propagated to the signal model representing the energy scale uncertainty for barrel/endcap and converted/unconverted photons.

\subsubsection{Photon energy resolution}
Section~\ref{sec:photon_energy} also describes the method used to smear the \MC events such that the energy resolution matches that observed in data using the \Zee control sample. As above there are four nuisance parameters representing the constant smearing uncertainty for each of the 7 and 8~\TeV datasets for barrel/endcap and converted/unconverted photons. For converted/unconverted photons in the barrel there is an additional pair of uncertainties for the stochastic smearing term. Given the low sensitivity of the endcap region and the low statistics in the 7 TeV sample this additional stochastic parameterisation is only used in the barrel at 8~TeV.

\subsubsection{Energy scale uncertainty due to differences between electron and photon reconstruction}
As the uncertainties, and the corresponding corrections, to the energy scale and resolution described above are derived from the \Zee decays, an important source of systematic uncertainty arises from those differences between electrons and photons which are not well modelled by the \MC simulation. The important effect is not the absolute difference between electrons and photons, which is clearly different, but the differences between them in data which are not modelled accurately by the simulation. 

The effect which is most significant is due to an imperfect description of the tracker and services material between the beampipe and the \ECAL in the simulation. Studies of photon conversions, electron bremsstrahlung and pion scattering suggest a material deficit in the simulation of up to 20\% in some areas upstream of the \ECAL. The uncertainty on the energy scale is assessed by using specialised \MC samples in which the tracking material budget is increased by 10\% (in the barrel) and 20\% (in the endcap) in 8 categories (4 in \eta $\times$ 2 in \rnine). Two nuisance parameters, which get correlated across the 7 and 8~\TeV datasets, are implemented representing the energy scale uncertainty due to the material mismodelling in the central barrel, $|\eta|<1$, and the rest of the ECAL, $|\eta|\geq1$.

A further difference between electrons and photons which is not modelled in the simulation is the variation in the amount of scintillation light which reaches the photodetector given the longitudinal position in the crystal at which the light is emitted. Typically the peak position of the amount of scintillation light for electron showers is earlier in the crystal than photon showers and this is assumed as identical in the simulation. A single correlated nuisance parameter is applied to account for this effect.

Finally, one further systematic is applied to account for improvements made in more recent versions of the simulation which model electron bremsstrahlung at low \pT and photon conversion at high \pT much better. An additional correlated nuisance parameter is applied to account for this effect.

\subsubsection{Photon preselection efficiency}

The efficiency of the preselection described in Sec.~\ref{sec:photon_presel} is measured using \Zee decays in data and \MC simulation. The \MC efficiency is then corrected such that it matches the data and this incurs a systematic uncertainty which gets applied as an overall normalisation on the expected signal event yield in each analysis category. 

\subsubsection{Photon identification BDT}

The photon identification BDT output (described in Sec.~\ref{sec:pho_id_mva}) is an important input to the diphoton BDT which is used to classify events. A shift in the photon ID BDT output, due to inaccurate simulation in the training samples, can have a direct effect on the output of the diphoton event BDT which in turn will lead to an event being misclassified; either it will fail the lower edge cut or it will go into a different analysis category. By examining the variation in the photon ID output for \Zee electrons, \Zmumugamma photons and diphoton events in data with \mgg$>160$~\GeV it is found that applying a systematic shift of 0.01 to the output covers any discrepancies between data and \MC simulation. This shift is applied and then propagated through to the diphoton \BDT which in turn results in an uncertainty implemented as a relative yield change across event class (in other words: a category-migration).

\subsubsection{Photon energy resolution estimate}

The most important input to the diphoton BDT, i.e.\ the variable with the most discriminating power, is the photon energy resolution estimate. As for the photon ID systematic above, the differences between data and \MC are studied for \Zee decays, \Zmumugamma decays and high \pT photons. A systematic variation of the nominal value is applied and propagated through to the diphoton \BDT output, after which the uncertainty is implemented as an analysis category-migration.

\subsection{Systematic uncertainties related to diphoton events}

\subsubsection{Vertex efficiency}

The vertex efficiency in \MC events is corrected by a scale factor obtained from the vertex efficiency ratio between data and \MC as measured in \Zmumu events where the muon tracks are ignored (to replicate the situation with photons). The uncertainty associated with this is implemented by changing the relative fraction of the right/wrong vertex signal shape. 

\subsubsection{Trigger efficiency}

The trigger efficiency in \MC events is corrected to match the data using \Zee events with a tag and probe technique~\cite{tag_and_probe}. The uncertainty associated to this is implemented as an overall normalisation change in each category.

\subsubsection{Mass scale non-linearity}

Another important consideration when using the \Zee decay as a calibration and control tool in the analysis is not only the differences between electrons and photons but also the difference in the mass scale between the $Z$ ($m_{Z}=91.2$~\GeV~\cite{pdg}) and a Higgs boson at $\mH=125$~\GeV. Although the absolute difference is not important, the relative differences between data and \MC simulation are. This effect is measured by comparing the energy measured from the electromagnetic shower alone and the \pT from the tracker of electrons as a function of the scalar \ET sum of the two electrons in \Zee decays. This uncertainty is applied as a global mass shift,  which has a correlated effect on the signal position in each category.

\subsubsection{Uncertainty on $Z$ mass}

Given that most of the systematics on the energy scale are obtained from studies on the $Z$ decay to electrons, the uncertainty on the $Z$ mass measured in this decay is also included as a systematic. It has a correlated effect across all categories.

\subsubsection{Luminosity measurement}

The luminosity measurement and its uncertainty is a \CMS universal value and is described in Refs.~\cite{lumi1,lumi2}.

\subsubsection{Theoretical cross section and branching ratio}

The uncertainties on the theoretically predicted \SM Higgs cross section and its branching fraction to two photons are implemented following the recommendations of the LHC Higgs Cross Section Working Group~\cite{LHCHiggsCrossSectionWorkingGroup3}. 

\subsection{Systematic uncertainties related to production mode tagged classes}

\subsubsection{Jet tagging efficiency}

One of the largest uncertainties for measurement of the respective couplings to fermions and bosons of the Higgs boson originates from the complexity in isolating events produced by gluon fusion and those produced by vector boson fusion. As described in Sec.~\ref{sec:exclusive_tags} this is done by tagging jets characteristic of \VBF Higgs decays. However there is a large uncertainty involved when additional jets are produced by gluon fusion-induced Higgs decays. Using the Stewart-Tackmann procedure~\cite{vbf_syst} the uncertainty on the yield of gluon fusion events in the \VBF tagged event classes is calculated and implemented as a category-migration systematic. There are further effects from the jet energy scale, jet identification efficiency and the efficiency of rejecting jets incorrectly identified as pileup jets which are taken from Refs.~\cite{jet_energy_corrections,jet_energy_corrs2}. These affect the relative yields of the signal expectation between the \VBF categories and also from the \VBF categories as a whole to the inclusive categories.  

\subsubsection{Lepton, \MET and $b$-tagging efficiency}
Additional normalisation uncertainties are applied to the exclusive mode categories which account for the efficiency of reconstructing and tagging leptons, \MET and $b$-jets.

\subsubsection{\ttH multijet rate}
The effect of the jet identification efficiency on the \ttH multijet category. Although this systematic is quite large it only has an effect on the expected yield in this one category.

\subsection{Summary}

A summary of all the systematic uncertainties is shown in Table~\ref{tab:systematics}. A summary of the systematic uncertainties which affect the signal strength measurement, and the size of their effect, is shown in Table~\ref{tab:systematics_mu}. A summary of the systematic uncertainties which affect the mass measurement, and the size of their effect, is shown in Table~\ref{tab:systematics_mh}.

\input{analysis/plots/my_systematics_table}

\begin{table}
\caption[Magnitude of the uncertainty on the signal strength, $\musm$, induced by the systematic uncertainties on the signal]{Magnitude of the uncertainty on the signal strength, $\musm$, induced by the systematic uncertainties on the signal.}
\begin{center}
\begin{tabular}{ l c }
\hline
\multirow{2}{*}{\textbf{Systematic uncertainty}} & \textbf{Uncertainty (\%)} \\
 &  \textbf{on $\musm$} \\
\hline
\hline
%%%%%% per photon ----------------
Energy scale and resolution corrections & 0.02\\
Uncertainty from resolution estimate and photon identification \BDT & 0.06\\
Other experimental uncertainties & 0.04\\
Theoretical uncertainties & 0.11\\
\hline
\hline
All systematic uncertainties on the signal model & 0.13 \\
\hline
\end{tabular}
\end{center}
\label{tab:systematics_mu}
\end{table}

\begin{table}
\caption[Magnitude of the uncertainty on the signal position, $\mH$, induced by the systematic uncertainties on the signal]{Magnitude of the uncertainty on the signal position, $\mH$, induced by the systematic uncertainties on the signal.}
\begin{center}
\begin{tabular}{ l c }
\hline
\multirow{2}{*}{\textbf{Systematic uncertainty}} & \textbf{Uncertainty (GeV)} \\
 &  \textbf{on $\mH$} \\
\hline
\hline
%%%%%% per photon ----------------
Energy scale and resolution corrections & 0.05 \\
Non-linearity extrapolation from $Z$-boson scale to Higgs scale & 0.10 \\
Differences between electrons and photons & 0.11 \\
\hline
\hline
All systematic uncertainties on the signal model & 0.16 \\
\hline
\end{tabular}
\end{center}
\label{tab:systematics_mh}
\end{table}

\section{Statistical interpretation of the data}
\label{sec:stats}
To aid with statistical interpretation of the data it is useful to define two hypotheses with which the data can be compared. These are the background only model and the signal plus background model, known as the null hypothesis, $H_{0}$, and the alternate hypothesis, $H_{1}$, respectively. The background model is obtained from the data by profiling the different parametrisation choices, as per the ``envelope" method described in Sec.~\ref{sec:envelope}. The benchmark used for the signal model is the \SM Higgs expectation as described in Sec.~\ref{sec:signal_model} which can be expressed in terms of two physical parameters of interest; the signal strength relative to the \SM expectation, $\mu=\musm$, and the signal position or Higgs mass, \mH. The likelihood function for statistical interpretation can be written as,
\begin{equation}
  \mathcal{L}(\mu,\mH;\theta,\theta_{b},\theta_{d}|\mgg) = \mu\cdot\mathbf{S}(\mgg|\mu,\mH;\theta) + \mathbf{B}(\mgg|\theta_{b},\theta_{d}),
  \label{eq:likelihood}
\end{equation}
where \mgg is the diphoton invariant mass, $\theta$ are continuous nuisance parameters affecting the signal model, $\theta_{b}$ are continuous nuisance parameters affecting the background shape, $\theta_{d}$ are discrete nuisance parameters dictating the background function and $\mathbf{S}$ and $\mathbf{B}$ are the probability distribution functions for the signal and background respectively. In the case of the \SMVA there is no dependence on \mgg as events are simply counted in the analysis bins. It can be seen that $\mu$ is a continuous parameter which represents the size of the fitted signal. The null hypothesis, or background only model, is a simple case of the full model where $\mu=0$.

When determining the best fit values of parameters in our model and their errors, the full likelihood function, as expressed in Eq.~\ref{eq:likelihood}, is fit to the data and the \NLL is scanned as a function of the parameter(s) of interest. This is a standard statistical procedure for measuring the value and error of a model parameter~\cite{FredJames}. An important measurement to make of the Higgs boson is its signal strength, relative to the \SM expectation, when considering fermionic production modes (\ggH and \ttH) and bosonic production modes (\VBF and \VH) separately. This can help to ascertain the relative coupling strength of the Higgs to fermions and bosons. Given that the signal model is already expressed as a sum over the different production processes one can redefine the likelihood in this case so that the signal strength is split into two production dependent parameters, \RF and \RV:
\begin{align}
  \mu\cdot\mathbf{S}(\mgg|\mu,\mH;\theta) & = &  \RF \cdot\biggl(\mathbf{S_{ggH}}(\mgg|\mu,\mH;\theta)+\mathbf{S_{ttH}}(\mgg|\mu,\mH;\theta) \biggr) \nonumber \\
 & & + \RV \cdot\biggl(\mathbf{S_{qqH}}(\mgg|\mu,\mH;\theta)+\mathbf{S_{VH}}(\mgg|\mu,\mH;\theta)\biggr)
 \label{eq:rvrf}
\end{align}
where $\mathbf{S_{proc}}$ represents the signal model for the process ``proc" only.

There are two important statistical tests that are performed aside from ascertaining the best fit values of parameters in the model. The first is an exclusion test designed to reject the alternate hypothesis in data. The second is a probability, or $p$-value, test designed to ascertain the likelihood of the null hypothesis fluctuating to give a signal. This requires the definition of a test statistic,
\begin{equation}  
  q_{\mu} = 
  \begin{cases}
    -2\ln\frac{\mathcal{L}(\mathrm{data}|\mu,\mH,\hat{\theta}_{\mu})}{\mathcal{L}(\mathrm{data}|\hat{\mu},\mH,\hat{\theta}_{\hat{\mu}})} & 0\leq\hat{\mu}\leq\mu \\
    0 & \hat{\mu}<0
  \end{cases},
  \label{eq:teststat}
\end{equation}
where $\hat{\theta}_{\mu}$ and $\hat{\theta}_{\hat{\mu}}$ represent the nuisance parameters at their best fit values given a particular value of and at its global best fit value $\hat{\mu}$~\cite{asymptotic_form}. The statistical tests which require $q_{\mu}$ are carried out at a specific hypothesised Higgs mass and so \mH is set to a particular value when defining the test statistic. It can be shown that $q_{\mu}$ is the most powerful test of hypothesis $H_{0}$ against hypothesis $H_{1}$ and this is known as the \textit{Neymann-Pearson Lemma}~\cite{FredJames}.

For exclusion limits the $CL_{s}$ method is used, which is designed to give less stringent limits in situations where there is little discriminatory power between the null and alternate hypotheses~\cite{cls}. The $CL_{s}$ exclusion requires calculation of two $p$-values given the test statistic distribution of the two hypotheses is known, as shown in Eqs.~\ref{eq:cls1}-~\ref{eq:cls3}. A common way of obtaining this distribution is by generating pseudo-data under each hypothesis. However asymptotic approximations are used to avoid the computational overhead of generating toy experiments~\cite{asymptotic_form}. A one-sided upper limit is determined on $\mu$ by enforcing the constraint that $\hat{\mu}\leq\mu$ and the exclusion power of the limit is given by $1-CL_{s}$. The definition of $CL_{s}$ is 
\begin{align}
  & CL_{s} = \frac{CL_{s+b}}{CL_{b}} \label{eq:cls1}, \\
  & CL_{s+b} = \int_{q_{\mu}^{obs}}^{\infty}f(q_{\mu}|\mu=\hat{\mu})dq_{\mu} \label{eq:cls2}, \\
  & CL_{b} = \int_{q_{\mu}^{obs}}^{\infty}f(q_{\mu}|\mu=0)dq_{\mu} \label{eq:cls3}, 
\end{align}
where $q_{\mu}^{obs}$ is the observed value of the test statistic in data and $f(q_{\mu}|\mu)$ is the distribution of the test statistic for the hypothesis with value $\mu$. The $CL_{s}$ exclusion method is demonstrated by an example in Fig.~\ref{fig:cls}. In this case a \SM Higgs boson with values of signal strength $\mu>0.8$ is excluded at 92.48\%. In reality the desired exclusion level is 95\% and consequently the value of $\mu$ is adjusted until the $CL_{s}$ reaches 0.05 and then all values of $\mu$ greater than or equal to this value are considered as excluded. When referring to the ``expected" exclusion (as opposed to the ``observed" exclusion just explained) the observation value is considered as being at the mean of the null hypothesis test statistic distribution.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{analysis/plots/testStatDrawing.pdf}
    \caption[A demonstration of the test statistic distribution]{A demonstration of the test statistic distribution for the background only model (red) and the signal plus background model (blue) and the observed value in data (black line). The exclusion power is given by $1-CL_{s}$, in this case values of $\mu>0.8$ are excluded at 92.48\% confidence.}
    \label{fig:cls}
  \end{center}
\end{figure}

When quantifying the significance of an observed excess the test statistic $q_{0}$ is used which is obtained by setting $\mu=0$ in Eq.~\ref{eq:teststat}. The requirement that $\hat{\mu}\geq0$ ensures that only positive excesses are considered significant. The probability that the background-only hypothesis is rejected in favour of the signal plus background hypothesis is given in terms of the $p$-value, $p_{0}$, defined as
\begin{equation}
  p_{0} = \int_{q_{0}^{obs}}^{\infty}f(q_{0}|\mu=0)dq_{0}.
  \label{eq:pvalue}
\end{equation}
Typically a value of $p_{0}\leq1.3\times10^{-3}=3\sigma$ is enough to claim observation of new physics while $p_{0}\leq2.87\times10^{-7}=5\sigma$ is enough to claim a discovery of new physics.

In the spin analysis the signal model is parametrised in terms of $\mu$, \mH and two additional parameters, $x$ and \fqqbar, which dictate the amount of signal from spin-2 and the fraction of spin-2 from $q\bar{q}$ production. The signal model parametrisation can be written as
\begin{equation}
  \textbf{S}(m_{H},fqq,x;\theta) = (1-x)\cdot\boldsymbol{S_{SM}}(m_{H};\theta)  + x\cdot\Bigl[f_{q\bar{q}}\cdot \boldsymbol{S_{q\bar{q}}}(m_{H};\theta) + (1-f_{q\bar{q}})\boldsymbol{S_{gg}}(m_{H};\theta)\Bigr].  
  \label{eq:spin_sig}
\end{equation}

