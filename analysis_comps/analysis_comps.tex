\chapter{Common analysis components}
\label{chap:common_analysis_components}

This thesis describes three complementary analyses in the Higgs to two photons search at CMS. These differ in their photon selection, event selection, event classification (or categorisation) and statistical methods for extracting results. They are described in the following chapter (Chapter~\ref{chap:selection_and_categorisation}). However, there are many components which they share. These are detailed below.

As we have seen in Eq.~\ref{eq:invmass} and~\ref{eq:dipho_inv_mass}, repeated below in Eq.~\ref{eq:invmass2} for convenience, the diphoton invariant mass is constructed from the two photon energies and the angle between them.
\begin{equation}
  m_{\gamma\gamma} = \sqrt{2E_{1}E_{2}(1-\cos\alpha)}
  \label{eq:invmass2}
\end{equation}
Consequently, important considerations for this analysis are photon energy resolution and good opening angle resolution. The latter is completely dominated by the vertex resolution, as the position resolution of the photons (the location at which they hit the \ECAL) is negligible in comparison. Details of how this is exploited in the analyses are given at the end of this chapter in Sections~\ref{sec:photon_energy} and~\ref{sec:vtx_reco}. The selection of events is described in the chapter after this (Chapter~\ref{chap:selection_and_categorisation}) alongside the categorisation, or binning, scheme whereby events which share similar signal to background ratios are collected into different classes of event, which take advantage of areas of phase space which share similar signal to background ratios.
After a preliminary discussion of multivariate analysis techniques, the datasets, the triggering and the \MC simulation are discussed.

\section{Boosted Decision Trees}
\label{sec:bdts}
Multivariate analyses (\acs{MVA}) are commonly used in High Energy Physics analyses to extract the maximum possible signal sensitivity in cases where the background rates are high. The advantage of \MVAs is that given a set of input variables a selection scheme can be built, to classify or correct events, in a multidimensional phase space to exploit differences between the signal and background in these variables and importantly in the correlations between them. A particular type of \MVA which is used widely in this analysis is the \BDT. \BDTs are preferred because they are more robust to the inclusion of variables which have little or no discriminating power. There are two broad types of \BDT used, one is known as a regression \BDT and the other as a classification \BDT~\cite{bdt,bdt2,bdt3}.

\subsection{Classification \acs{BDT}}
A classification \BDT will, given a set of input variables, assign a value (typically between $-1$ and $1$) to each event based on how signal-like that event is. This serves to collapse all the event information into one discriminating variable which can be used to classify differences between the signal and background. The input is provided as the probability distributions (which can be supplied as binned or unbinned data samples or as a functional form) of the background and signal for a set of ``input variables". The process involves construction of a series of \aclp{DT} (\acs{DT}) complemented by a ``boosting" step which serves to mitigate against ``overtraining" on fluctuations within the training samples. This analysis chooses a particular type of decision tree boosting known as ``gradient" boosting because it is more robust against outliers or mislabelled data points~\cite{TMVA}.

The \DT is built by applying sequential cuts to the input variables and assessing the relative signal purity, $p$, in the sub-sample remaining after each cut
\begin{equation}
  p = \frac{N_{s}}{N_{s}+N_{b}},
\end{equation}
where $N_{s}$ and $N_{b}$ are the sum of weights of the signal and background remaining in each sub-sample. A threshold criterion, known as the Gini index~\cite{TMVA} $p(1-p)$, is applied to decide whether to split the sample further. The process continues and the splitting is curtailed when either the threshold or the user defined maximum tree depth (number of subsamples allowed) is reached. The value of each cut is varied such that the signal purity, $p$, in each sub-sample is maximised. An event is assigned a value of $-1$ or $+1$ depending on whether it falls into a sub-sample with $p>$0.5 or not. Clearly some fraction of events will be misclassified where the actual number which get misclassified will depend on the discriminatory power available from the chosen input variables. In order to reduce this effect a series of \DTs are trained and each assigned a weight derived by the ``boosting" process.

We assign each \DT as a member of a family of $M$ functions, $f(\vec{x};\vec{a}_{m})$, which depend on the input variables, $\vec{x}$, and the set of cuts in that tree, $\vec{a}_{m}$. The object is to construct an overall decision tree which consists of the weighted average of each \DT,
\begin{equation}
  F(\vec{x};\vec{\beta},\vec{a}) = \sum_{m=0}^{M} \beta_{m}f(\vec{x},\vec{a}_{m}) \;\;\;\;\; \textrm{where} \;\; \vec{\beta} = (\beta_{0},\beta_{1}...\beta_{M}).
\end{equation}
For the ``adaptive" boost algorithm the input events for training the proceeding tree are reweighted by the fraction that get misclassified in the previous tree. That is, the background events get reweighted by the total fraction of background events which were classified as signal in the previous tree and the signal events get reweighted by the total fraction of signal events which were classified as background in the previous tree. In the ``gradient" boosting procedure, which applies to the majority of \BDTs used in this thesis, the weight for successive trees is obtained by minimising the deviation in the loss function (Eq.~\ref{eq:bdt_loss_fcn}) each time a new tree is added\footnote{ Friedman showed that for certain choices of loss function the ``gradient" boost and ``adaptive" boost procedures are identical, although this is not the case for a general loss function, nor the loss function of the form used in this thesis, shown in Eq.~\ref{eq:bdt_loss_fcn}~\cite{bdt3}.}. The loss function, between the weighted tree response $F(\vec{x};\vec{\beta},\vec{a})$ and the true output $y$ obtained from the training sample, in this case is
\begin{equation}
  L(F,y) = \ln(1+e^{-2F(\vec{x})y}).
  \label{eq:bdt_loss_fcn}
\end{equation}
A common procedure when constructing a \BDT to check for overtraining is to split both the background and signal into two independent samples. One is used to \emph{train} the \BDT and one is used to \emph{test} the response of the output. Clearly one requires that both the training and independent test sample look the same in the output variable. This is usually quantified by use of a Kolmogorov-Smirnov test, which broadly speaking ascertains the probability that the training and test samples originate from the same underlying distribution~\cite{kol_smir}.

In this way the output of $F(\vec{x};\vec{\beta},\vec{a})$ for a classification \BDT will be a ``semi-continuous" output\footnote{ In the sense that each decision tree will give a discrete output of either signal-like or background-like such that the boosted output of the numerous trees in the forest contains several hundred (depending on the number of trees) discrete values in the range $[-1,1]$.} from $-1$ to $+1$ with signal events in general given a higher score than background events.

\subsection{Regression \acs{BDT}}
A regression \BDT is used to estimate the true value of some variable given the values and correlations of several other variables. They are commonly used for correcting the energy of a particular object, for example a photon. Given a \MC source of photons the ``true" energy is regressed from the position, shape and raw energy of the supercluster. For regression \BDTs the output $F(\vec{x};\vec{\beta},\vec{a})$ represents the estimated corrected energy and the boosting procedure targets minimising the deviation between this and the true energy in \MC events.

\section{Data samples and triggering}

The data consists of two independent samples of proton-proton collisions collected by the CMS experiment at the \LHC in 2011 and 2012 with a centre-of-mass energy, $\sqrt{s}$, of 7 and 8 TeV, respectively. The total integrated luminosity of the two samples is 5.1~\fb and 19.7~\fb in 2011 and 2012, respectively, and they are collectively referred to as \LHC Run 1. The response of the detector has changed considerably over this period and much of the variation is modelled by the \MC simulation. Figure~\ref{fig:intlumi} shows the integrated luminosity delivered to and recorded by \CMS during \LHC Run 1.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/int_lumi_2011.pdf}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/int_lumi_2012.pdf}
  \caption[The total integrated luminosity delivered to and recorded by \acs{CMS} during the 2011 and 2012 run periods]{The total integrated luminosity delivered to and recorded by CMS during the 2011 (left) and 2012 (right) run periods. Due to down time of various subsystems in \CMS during run periods, particularly the \ECAL, the recorded luminosity given here is not exactly equivalent to the integrated luminosity of the datasets used in the analysis~\cite{lumi1,lumi2}.}
  \label{fig:intlumi}
\end{figure}

Events are selected for the analysis by requiring they pass an asymmetric diphoton trigger with \ET thresholds of 26 (18) and 36 (22) GeV for the leading (trailing) photon in the 2011 and 2012 runs, respectively. The candidates are also required to have either a high value of \rnine or to pass a loose calorimetric identification and isolation requirement. High trigger efficiency is achieved by selecting photon candidates which pass either requirement. The efficiency of the trigger for the analysis preselection is 99.5\%.

\subsection{Monte Carlo Simulation}
\label{sec:mc}

Accurate simulation of detector effects and efficiency is highly important. Knowledge of the expected Higgs signal shape is clearly essential and although the size and shape of the \mgg background is entirely data driven when extracting results, simulating the kinematics, shower shape and resolution properties of the background is important when training the selection and optimising the categorisation of events.

As explained in Chapter~\ref{chap:theory} the two main production mechanisms for a \SM Higgs boson at the \LHC are \ggH and \VBF. Typically the latter is produced at much higher Higgs \pT and this feature is exploited in the analysis (see Fig.~\ref{fig:gen_level}). Consequently, it is important to model the \pT distribution of these two production modes accurately. The signal samples for these two processes are generated using \POWHEG~\cite{powheg1,powheg2} at NLO interfaced with \PYTHIA~\cite{pythia} including a reweighting factor which matches their \pT spectrum to that when including the NNLO and NNLL terms. For the associated production modes, with a $W^{\pm}$, $Z$ or $t$ quarks, (\VH and \ttH) only \PYTHIA is used. The \SM Higgs boson cross sections and branching fractions, and their uncertainties, are taken from Ref.~\cite{LHCHiggsCrossSectionWorkingGroup3}

The spin-2 graviton with minimal couplings, \graviton, has two production mechanisms, one via gluon-fusion ($ggX$) and one via quark-antiquark annihilation ($q\bar{q}X$). The graviton samples are generated using the \JHU generator~\cite{jhu}. In these samples the signal events are reweighted such that the graviton \pT spectrum matches the Higgs \pT spectrum in the \SM signal samples. The kinematic properties of a spin 2 \textit{graviton-like} Higgs are not well defined and can depend on the specifics of the model in question. Matching the \pT spectrum of the spin-2 samples with the \SM spin-0 ensures that no discrimination arising from model dependence of the \pT distribution will arise.

The simulated background samples are used solely for cut and category optimisations and training of multivariate discriminants. The background which contains the \QCD continuum of prompt photons (referring back to Chapter~\ref{chap:theory} these are produced by Born and box diagrams) is simulated using \SHERPA~\cite{sherpa} at 8~\TeV and \MADGRAPH~\cite{madgraph} at 7~\TeV. The prompt-fake and fake-fake backgrounds, in which one or both photons are faked by a neutral meson (usually a \pizero) reconstructed as a photon, are generated using \PYTHIA. Samples of \Zee, \Zmumu and \Zmumugamma used for data/MC comparisons are generated with \POWHEG.

All of these \emph{generator level} samples are then run through the full \CMS detector simulation using \GEANT~\cite{geant}. This includes the effect of overlapping vertices (pileup) and detector effects (such as noise and crystal degradation) in four time periods; Run2011 (5.1~\fb), Run2012AB (5.3~\fb), Run2012C (7.1~\fb) and Run 2012D (7.3~\fb).


\subsection{Pileup and beamspot reweighting}
\label{sec:pileup_beamspot}

An important difference between the simulated samples and the data which can have a large impact on the analysis is the distribution of the number of primary vertices. The \emph{pileup} in the event affects many important analysis variables, for example photon shower shape and photon isolation as well as the diphoton invariant mass if the chosen vertex is wrong. Consequently the \MC events are reweighted such that the pileup distribution matches that in data. The reweighting technique is validated using \Zmumu events as shown in Fig.~\ref{fig:pileup} for the 7 and 8~\TeV samples.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/nvtx_zmumu_2011.pdf}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/nvtx_zmumu_2012.pdf}
  \caption[Distribution of the number of reconstructed vertices]{Distribution of the number of reconstructed vertices in the 2011 (left) and 2012 (right) run periods. Calculated using the Deterministic Annealing algorithm in~\cite{deterministic_annealing} for \Zmumu events in data (black dots) and MC events (red histogram) after reweighting.}
  \label{fig:pileup}
  \end{center}
\end{figure}

When the chosen vertex is incorrect the mass resolution is dominated by the spread in position of the pileup vertices (known as the beamspot width, $\sigma_{z}^{beamspot}$). Accurate modelling of this spread is important so that the resolution of wrong vertex events in simulation matches that in data. The \MC sample overestimates the beamspot spread by some 20\% so a simple reweighting is implemented for \MC events in which the wrong vertex is chosen (as the effect is negligible for events in which the chosen vertex is correct) such that the distribution of the distance between the chosen vertex position and the true vertex position, $\delta z=z_{chosen}-z_{true}$, match between data and \MC. The effect with and without reweighting compared to data is shown in Fig.~\ref{fig:beamspot}.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.55\textwidth]{analysis_comps/plots/beamspot.pdf}
  \caption[Distribution of $\Delta z$ (the distance between the chosen vertex and the true vertex in the $z$ direction)]{Distribution of $\Delta z$ (the distance between the chosen vertex and the true vertex in the $z$ direction) for data (black), the \MC simulation (red) and the \MC simulation after beam spot reweighting (green) for \Zmumu events.}
  \label{fig:beamspot}
  \end{center}
\end{figure}

\section{Energy measurement of photons}
\label{sec:photon_energy}

The photon energy obtained from the supercluster sum described in Section~\ref{sec:ecal}, even when including the intercalibration and transparency corrections shown in Fig.~\ref{fig:ecal_laser_corrs}, does not give the most optimal resolution for the energy measurement of photons at \CMS. On top of this energy (known as the \emph{raw} supercluster energy, $E_{raw}$) it is also valuable to correct for additional energy losses. These arise from bremsstrahlung; where the photon converts in the material upstream of the \ECAL and the two electrons radiate additional photons and thus some of the photon shower is missed, and from local non-containment of the shower; where some energy is lost due to small gaps between \ECAL crystals and larger gaps between ``modules" or sections of crystals. These corrections are obtained using a specialised regression \BDT (see Sec.~\ref{sec:bdts}) trained on a \MC source of prompt photons from a sample containing photons and jets. The \BDT targets accurate measurements of individual photons' energies by correcting the raw supercluster energy and provides an estimate for the energy resolution of each photon given the position and shower shape of the supercluster. The training is done separately for barrel and endcap photons (as the cluster shapes look very different for these two distinct regions) and is also performed separately for the 7 and 8 TeV datasets. The following input variables are used:
\begin{itemize}
  \item the global position of the supercluster in \eta and \phi;
  \item a collection of shower shape variables which aim at providing information on the likelihood and location of a photon conversion and the degree of showering in the material:
  \begin{itemize}
    \item the \rnine of the supercluster (as previously described in Section~\ref{sec:ecal});
    \item the ratio of the 5$\times$5 crystal energy to the raw supercluster energy (equivalent to $R_{25}$);
    \item the energy weighted \eta-width and \phi-width of the supercluster (in other words the spread of the shower);
    \item the number of basic clusters in the supercluster;
    \item the ratio of energy in the \HCAL behind the supercluster to the \ECAL energy of the supercluster, $H/E$;
    \item the ratio of the preshower energy to the raw supercluster energy (endcap only).
  \end{itemize}
  \item a collection of the seed crystal and the seed cluster variables which aim at providing information about energy lost through gaps and cracks between crystal and crystal modules:
  \begin{itemize}
    \item the relative energy and position of the seed cluster;
    \item the local energy covariance matrix;
    \item energy ratios between the seed and the 3$\times$3 and 5$\times$5 areas around the seed;
    \item the \eta and \phi index of the seed crystal and the position of the seed cluster relative to the crystal centre.
  \end{itemize}
  \item additionally, the number of primary vertices and the median energy density, \rho, (see Sec.~\ref{sec:pileup}) are included to account for residual energy scale effects from pileup.
\end{itemize}
The regression is trained using an additional feature to that described in Sec.~\ref{sec:bdts} whereby the target is to predict the full probability distribution of the ratio of the true energy to the raw energy, $E_{true}/E_{raw}$. The target is a double Crystal Ball~\cite{CrystalBallShape} distribution which consists of a Gaussian core and power law tails on either side. This can be fully parametrised by six variables, the Gaussian mean and width ($\mu$, $\sigma$), the power parameters ($n_{L}$, $n_{R}$) and the power law tail cutoff parameters ($\alpha_{L}$, $\alpha_{R}$). Each of these parameters has a non-parametric dependence on the input variables, $\vec{x}$, and this is \emph{learned} by the regression training whilst simultaneously minimising the likelihood,
\begin{equation}
  -\ln \mathcal{L} = - \sum_{MC \; photons} \ln p\bigl[E_{true}/E_{raw} | \mu(\vec{x}),\sigma(\vec{x}),\alpha_{L}(\vec{x}),\alpha_{R}(\vec{x}),n_{L}(\vec{x}),n_{R}(\vec{x})\bigr],
\end{equation}
for the double Crystal Ball distribution, $p$. The most probable value for the true energy estimate of each photon is then given by,
\begin{equation}
  E(\vec{x},E_{raw}) = \mu(\vec{x})E_{raw}
\end{equation}
and the per-photon energy resolution is given by,
\begin{equation}
  \frac{\sigma_{E}(\vec{x},E_{raw})}{E(\vec{x},E_{raw})} = \frac{\sigma(\vec{x})}{\mu(\vec{x})}.
\end{equation}
In this way the regression predicts the full probability distribution of $E_{true}/E_{raw}$ per photon given a particular configuration of the input variables, $\vec{x}$, and provides an estimate of the optimal energy correction and the energy resolution per photon. A comparison of this distribution with a statistically independent \MC sample is shown for the 8~\TeV training in Fig.~\ref{fig:regression_training}.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/regression_barrel_fix.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/regression_endcap_fix.pdf}
  \caption[A comparison of the predicted probability density of $E_{true}/E_{raw}$ from the regression training to the distribution in a statistically independent sample]{A comparison of the predicted probability density of $E_{true}/E_{raw}$ from the regression training (blue line) to the distribution in a statistically independent \MC sample (black points) for barrel photons (left) and endcap photons (right).}
  \label{fig:regression_training}
\end{figure}

\subsection{Correcting for residual discrepancies between data and Monte Carlo simulation}
\label{sec:scale_smearing}

After application of the energy regression correction there are some remaining discrepancies between data and \MC simulation. These residual effects are accounted for using \Zee events in data and simulation to correct the energy scale in the data and to apply an additional smearing term to the \MC events with systematic uncertainties propagated through the analysis to account for the uncertainties on these corrections.

\subsubsection{Energy scale corrections to the data}

The supercluster energy is identical for electrons and photons so by correcting the supercluster energy scale to a known source, namely the mass of the $Z$-boson, in dielectron decays the smaller residual energy scale effects are accounted for. When reconstructing the dielectron decays to derive these corrections, electrons are reconstructed as photons and the $Z$ mass calculated in the same way as for the diphoton invariant mass in Eq.~\ref{eq:dipho_inv_mass}, where the electron energy is obtained from the supercluster alone and the dielectron opening angle is obtained from the tracks. This can be done several times to account for various different effects. In the first stage scale corrections are derived in bins of time (run range) and \eta. After applying these corrections, further, much smaller, residual effects are accounted for in bins of \rnine (the size of the effect is different for converted and non-converted photons). After applying both of these a further step is taken for the 8 TeV data in the barrel to derive residual corrections in bins of \ET. Consequently the total scale correction is a product of three corrections in 59 bins of time $\times$ 4 bins in \eta $\times$ 2 bins in \rnine $\times$ 6 bins in \ET (where the last is applied for the 8~TeV barrel photons only).

The strategy for deriving these corrections is to take \Zee events in data and \MC simulation and extract the invariant dielectron mass in the relevant bin of interest. This mass distribution is fitted with a convolution of a relativistic Breit-Wigner (designed to handle the underlying Z line shape~\cite{pdg}) and a Crystal Ball function which models the calorimeter resolution effects and bremsstrahlung losses in the material upstream of the \ECAL. The Breit-Wigner parameters are fixed to the PDG values of $M_{Z}=91.188$~\GeV and $\Gamma_{Z}=2.495$~\GeV~\cite{pdg} whilst the Crystal Ball parameters which model the detector effects are allowed to float. The scale correction, $\Delta E$, is then defined as the relative difference between the Crystal Ball peak in data and simulation,
\begin{equation}
  \Delta E = \frac{m_{data}-m_{MC}}{M_{Z}}.
\end{equation}
\subsubsection{Energy resolution smearing for the Monte Carlo events}

A similar method is used to extract a smearing factor that can be applied to the \MC events such that the width of the invariant mass distribution in \Zee decays matches between data and \MC events. This is done in 4 bins of \eta $\times$ 2 bins of \rnine and is parametrised as the quadratic sum of two resolution components: a constant term, $\Delta C$, and a stochastic term, $\Delta S$, which aims to model the expected resolution effects explained in Eq.~\ref{eq:energy_res} in Sec.~\ref{sec:ecal}. The smearing term, $\Delta\sigma$, is parametrised as,
\begin{equation}
  \Delta\sigma = \frac{\Delta S}{\sqrt{E_{T}}} \oplus \Delta C.
\end{equation}
The effect of the scale and smearing corrections is shown for the \Zee data and \MC samples in Fig.~\ref{fig:scale_smearing_Zee}. Figure~\ref{fig:scale_smearing_analysis} shows the \Zee invariant mass distribution for data and \MC events at 8~TeV for events passing the analysis preselection. The discrepancy between the data and the \MC simulation in the tails of the dielectron invariant mass distribution, shown in the right hand plot of Fig.~\ref{fig:scale_smearing_analysis}, is considerably reduced (from $\sim20\%$ to $\sim10\%$) after the full analysis event selection is made (not just the preselection shown here). Furthermore, the analysis sensitivity is decreased by less than 5\% when events with at least one photon in the endcap are removed and consequently the remaining discrepancy is not considered as significant. Each of the scale and resolution corrections has an associated uncertainty and these uncertainties are propagated per photon through the analysis. There are also additional uncertainties included which account for differences between electrons and photons and the difference between the $Z$ mass scale (around 90~\GeV) and the Higgs mass scale (around 125~\GeV). Systematic uncertainties are described in more detail in Sec.~\ref{sec:systematics}.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/zee_beforecorr_fix.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/zee_aftercorr_fix.pdf}
  \caption[The \Zee invariant mass shape before and after scale and smearing corrections are applied]{The \Zee invariant mass shape comparison of data and MC events before (left) and after (right) the scale and smearing corrections are applied. Shown for 8~TeV electrons (reconstructed as photons) in the barrel (left) and in the barrel with $R_{9}\geq0.94$ (right).}
  \label{fig:scale_smearing_Zee}
\end{figure}

\begin{figure}
  \includegraphics[width=0.99\textwidth]{analysis_comps/plots/massEBEE_fix.pdf}
  \caption[The \Zee invariant mass distribution at 8~TeV when the electrons are reconstructed as photons]{The \Zee invariant mass distribution at 8~TeV for electrons reconstructed as photons when both electrons are in the barrel (left) and when at least one is not in the barrel (right). Shown for data (black points) and \MC events (blue histogram) which pass the analysis preselection when the electron veto is inverted.}
  \label{fig:scale_smearing_analysis}
\end{figure}

\section{Vertex reconstruction}
\label{sec:vtx_reco}

The resolution on the opening angle has a negligible effect if the correct vertex can be found within 10~mm of the true interaction point. As seen in Sec.~\ref{sec:pileup_beamspot} the beamspot has an RMS spread of about 5~cm in the $z$ direction and there is an average of $\sim20$ vertices per bunch crossing. Because the beam direction is along the $z$-axis the spread of the vertex in the $x$ and $y$ directions is tiny ($<0.5$~mm) and consequently mismeasurement of the primary vertex in the $x$-$y$ plane is small and has no impact on the mass resolution. By assigning the correct vertex to the diphoton pair, using other information in the tracking system, most of the mass resolution can be preserved. The method used to extract the primary vertex is a classification \BDT which exploits the correlation between the diphoton pair and the recoiling tracks from the underlying interaction as well as additional information in the tracking system if there is a photon conversion pair. The output of this per-vertex \BDT is evaluated for each vertex in the event and the primary vertex is assigned as the one with the highest value of the \BDT output (i.e.\ the value nearest 1.). In addition, it is possible to construct another \BDT whose output is proportional to the probability that the chosen vertex is the correct one (described in Sec.~\ref{sec:bdt_prob}). This probability becomes a useful discriminating variable for the analyses later on.

The vertex \BDT uses the following input variables:

\begin{itemize}
  \item $\sum\limits_{i} |\vec{p}{}^{\;i}_{T}|^{2}$ - the sum of the transverse momentum squared of all of the tracks which originate from this vertex, representing how hard the interaction is at this vertex.
  \item $\frac{\vec{p}^{\gamma\gamma}_{T}}{|\vec{p}^{\gamma\gamma}_{T}|} \cdot \sum\limits_{i} \vec{p}_{T}^{i}$ - the dot product between the transverse momentum of the diphoton system and the sum of all other tracks originating from this vertex, representing the recoil of the tracks relative to the diphoton system.
  \item $\bigr(|\sum\limits_{i} \vec{p}^{\;i}_{T}| - |\vec{p}^{\gamma\gamma}_{T}|\bigl) / \bigr(|\sum\limits_{i} \vec{p}{}^{\;i}_{T}| + |\vec{p}^{\gamma\gamma}_{T}|\bigl)$ - the asymmetry between the diphoton system and the other tracks originating from this vertex.
  \item $|z_{v}-z_{c}|/\sigma_{c}$ - this is added for events which contain at least one photon conversion where $z_{v}$ is the $z$ position of the vertex in question and $z_{c}$ and $\sigma_{c}$ are the estimated $z$ position of the vertex from conversion information and its approximate error as defined below.
\end{itemize}

For events which contain at least one photon conversion, the conversion tracks and/or the conversion momentum can be used to point back to the beam line and estimate the vertex position. This can be achieved in one of two ways. In cases where the conversion occurs early, i.e.\ in one of the first layers of the tracking system, then the electron pair from the conversion will leave two clean and distinct tracks. This means that the momentum of the conversion pair can be accurately reconstructed and used to point from the conversion vertex position back to the beam line and thus the nearest primary vertex. In cases where the conversion occurs late in the tracking system, there are not enough track hits to accurately reconstruct the momentum of the conversion pair. However the incident position of the photon at the \ECAL face is well known in this case, so the line which connects the \ECAL position with the conversion vertex can be used to point back to the beam line. This is diagrammatically represented in Fig.~\ref{fig:conv_diags} for both cases.

\begin{figure}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/ConvDiagNewCase1.pdf}
  \includegraphics[width=0.49\textwidth]{analysis_comps/plots/ConvDiagNewCase2.pdf}
  \caption[A schematic showing the two methods used for locating the primary vertex]{A representation of the two methods for locating the primary vertex using photon conversion information. The left plot is for cases where the conversion occurs early enough in the tracker that the two electron tracks can be used to construct the converted pair momentum, which is then combined with the conversion vertex position to point back to the beam line. The right plot is for cases where the conversion occurs late in the tracker and the energy weighted supercluster position and the conversion vertex position are used to point back to the beam line.}
  \label{fig:conv_diags}
\end{figure}

For Case 1 conversions the primary vertex $z$ position is calculated as,
\begin{equation}
  z_{c} = z_{conv} - r_{conv}\cot(\alpha),
\end{equation}
where $z_{conv}$ is the $z$ position of the conversion vertex, $r_{conv}$ is the distance of the conversion vertex from the beam line and $\alpha$ is the angle between the beam line and the conversion momentum.

For Case 2 conversions the primary vertex $z$ position is calculated as,
\begin{equation}
  z_{c} = \frac{r_{conv}z_{SC} - r_{SC}z_{conv}}{r_{conv} - r_{SC}},
\end{equation}
where $z_{conv}$ and $z_{SC}$ are the $z$ positions of the conversion vertex and supercluster respectively, and $r_{conv}$ and $r_{SC}$ are the distance of the conversion vertex and the supercluster from the beamline.

There are six regions of the tracking system (refer back to Fig.~\ref{fig:cms_tracker}). When the conversion vertex is located in one of the inner regions; Pixel Barrel, Pixel Forward or TID, the Case 1
conversion information is included in the \BDT, otherwise the Case 2 conversion information is used. The resolution on the primary vertex position in conversions is estimated per tracking region using \gjet events in data for which the primary vertex efficiency is high and the photon converts. Using these events, the conversion resolution, $\sigma_{c}$, is calculated as the effective width\footnote{Half the narrowest interval which contains 68.3\% of the distribution} of the distribution of the difference, $\Delta z=z_{v}-z_{c}$, between the estimated $z$ position of the primary vertex without any conversion information, $z_{v}$, and the estimated $z$ position of the primary vertex when using conversion information alone, $z_{c}$. Consequently the fourth input variable to the \BDT, shown in the list above as $|z_{v}-z_{c}|/\sigma_{c}$, is effectively a pull distribution for the conversion vertex. The \BDT will favour vertices whose value of this variable is near zero.

The \BDT is trained on a sample of \Hgg \MC events. It is tested with a statistically independent sample and further validated using \Zmumu decays in data and \MC samples. The efficiency is measured in data using the \Zmumu channel where the muon tracks are removed from the \BDT variables to simulate a diphoton-like situation in data. The \BDT response is shown for \Zmumu data and \MC events for both the signal (right vertex) and background (wrong vertex) in Fig.~\ref{fig:vertex_bdt_response}. The chosen primary vertex is the one which gives the highest score \BDT output. The efficiency of the vertex selection as a function of the $Z$ \pT and the number of reconstructed vertices as measured in \Zmumu data and \MC samples is shown in Fig.~\ref{fig:vertex_bdt_efficiency}.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_output_7TeV_log.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_output_8TeV_log.pdf}
  \caption[The vertex \acs{BDT} response for \Zmumu events]{The vertex \BDT response for \Zmumu events in data (points) and MC events (filled histogram) for the primary vertex (green) and the background pileup vertices (red).}
  \label{fig:vertex_bdt_response}
\end{figure}

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_efficiency_pt_7TeV.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_efficiency_nvtx_7TeV.pdf} \\
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_efficiency_pt_8TeV.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vertex_bdt_efficiency_nvtx_8TeV.pdf}
  \caption[The efficiency of locating the correct vertex]{The chosen vertex efficiency as measured in \Zmumu data and \MC simulation as a function of $Z$ \pT (left) and number of reconstructed vertices (right) for the 7~TeV (top row) and 8~TeV (bottom row) data samples.}
  \label{fig:vertex_bdt_efficiency}
\end{figure}

\subsection{Estimating the per-event probability that the correct vertex is chosen}
\label{sec:bdt_prob}

The total efficiency for assigning the correct vertex using the method described in the preceding section is at the level of 75\% during 2012 running conditions, where the correct vertex is defined as being within 10~mm of the true vertex. This means that for around 25\% of preselected events the mass resolution is dominated by the vertex resolution. Consequently, it is important to ascertain the probability that the chosen vertex is the correct one. An additional specific \BDT is constructed to address exactly this topic. The input variables used for this \BDT are:

\begin{itemize}
  \item the \pT of the diphoton system;
  \item the number of vertices in each event;
  \item the value of the per-vertex \BDT described above, for the three vertices with the highest score;
  \item the $z$ distance, $\Delta z$, between the chosen vertex and the second and third choice vertices;
  \item the number of photon conversions used; either 0, 1 or 2.
\end{itemize}

There is a linear relation between the response of this \BDT and the correct vertex efficiency (or probability). This is used to analytically obtain the per-event correct vertex probability for a given event. Figure~\ref{fig:vertex_bdt_prob_efficiency} shows that this estimation reproduces the required vertex efficiency as a function of Higgs \pT and number of reconstructed vertices.

\begin{figure}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vtxProbPt.pdf}
  \includegraphics[width=0.48\textwidth]{analysis_comps/plots/vtxProbNvtx.pdf}
  \caption[A comparison of the true vertex efficiency and the average vertex probability]{A comparison of the true vertex efficiency (black points) and the average vertex probability (blue band) for a statistically independent \MC Higgs sample simulated with 2012 running conditions.}
  \label{fig:vertex_bdt_prob_efficiency}
\end{figure}

\section{Event preselection}
\label{sec:photon_presel}

A simple and loose preselection is applied to all photons before they enter the analysis. The preselection requirements are identical for all analysis approaches and are designed to remove some \emph{fake} photons whilst maintaining near 100\% trigger efficiency. The variables used for preselection are defined below and the preselection cuts are described in Table~\ref{tab:preselection}.

\begin{itemize}
  \item $H/E$ - The ratio of hadronic energy in the \HCAL tower behind the supercluster to the electromagnetic energy in the supercluster. Neutral jets which fake photons typically leave a fraction of their energy in the \HCAL so there is a requirement that the value of this variable is small.
  \item $\sigma^{2}_{i\eta i\eta}$ - The RMS spread of the shower in the $\eta$ direction. Multiple showers of a \pizero, or more than one \pizero, result in a wider shower in \eta (as the \pizero decay product photons are separated in space). This cannot be exploited in the \phi direction because conversion electrons get separated by the magnetic field, however single photons, even when converted, occupy a narrow region in \eta. The separation of the two photons from a \pizero is minimal when they share the energy equally and given that typically $p_{T}\gg m_{\pi}$ the separation is close to minimal for most \pizero decays. Taking the transverse plane in the barrel, the separation $d=2Rm_{\pi}/p_{T}$ (where $R$ is the radius of the barrel), which for $p_{T}=40$~GeV gives a value of $d=$8~mm$\sim0.006$ in $\eta$. By referring to Table~\ref{tab:preselection} it is clear that the preselection requirement is quite loose.
  \item $ISO_{ECAL}$ - The total \rho-corrected electromagnetic energy in a cone of radius 0.4 in (\eta,\phi) around the photon candidate - see Sec.~\ref{sec:iso}.
  \item $ISO_{HCAL}$ - The total \rho-corrected hadronic energy in a cone of radius 0.4 in (\eta,\phi) around the photon candidate - see Sec.~\ref{sec:iso}.
  \item $ISO_{Tracks}$ - The total \rho-corrected track energy in a cone of radius 0.4 in (\eta,\phi) around the photon candidate - see Sec.~\ref{sec:iso}.
  \item $ISO_{PFCh}$ - The total \rho-corrected particle flow charged hadron energy in a cone of radius 0.4 in (\eta,\phi) around the photon candidate - see Sec.~\ref{sec:iso}.
\end{itemize}

In addition to the above an electron veto is applied to prevent contamination of the photon sample with electrons which originate from Drell-Yan interactions. This is achieved by removing photon candidates whose supercluster is matched to an electron track which has no missing hits in the innermost tracking region.

\begin{table}
\noindent
  \begin{center}
  \caption{Preselection cut values.}
    \begin{tabular}{c | c c c c c c c c c c }
      \hline
                & \multicolumn{2}{c}{Barrel} & \multicolumn{2}{c}{Endcap} \\
      \hline
      $R_{9}$        & $H/E$ & $\sigma^{2}_{i\eta i\eta}$ & $H/E$ & $\sigma^{2}_{i\eta i\eta}$  \\
      $\le$ 0.9 & $<$ 0.075 & $<$ 0.014 & $<$ 0.075 & $<$ 0.034 \\
      $>$ 0.9   & $<$ 0.082 & $<$ 0.014 & $<$ 0.075 & $<$ 0.034  \\
      \hline
                & \multicolumn{4}{c}{Both Barrel and Endcap}\\
      \hline
      $R_{9}$        & $ISO_{ECAL}$ & $ISO_{HCAL}$ &  $ISO_{Tracks}$ & $ISO_{PFCh}$ \\
      $\le$ 0.9 &$<$ 4 GeV & $<$ 4 GeV & $<$ 4 GeV & $<$ 4 GeV\\
      $>$ 0.9   &$<$ 50 GeV & $<$ 50 GeV & $<$ 50 GeV & $<$ 4 GeV\\
      \hline
    \end{tabular}
  \label{tab:preselection}
  \end{center}
\end{table}

\section{Using $Z$ decays for validation and efficiency measurements}
\label{sec:zee}

Whilst no known ``standard candles" with high statistics exist for high \pT photons in the \LHC environment a powerful control source for the \Hgg decay in both data and \MC simulation is the \Zee decay. From an \ECAL interaction view point electrons are very similar to photons and the $Z$ is relatively near the relevant Higgs search range in mass. The differences between the $Z$ and the Higgs, in both their mass and \pT distribution, and also the differences between electrons originating from a $Z$ and photons originating from a Higgs are important systematic uncertainties on the Higgs mass scale and resolution. By inverting the electron veto usually applied in the preselection, the Higgs to two photon analysis can be identically replicated but with the very pure diphoton sample replaced with a pure dielectron sample. One additional process which can be used as a direct control for photons is the \Zmumugamma decay although the statistics, even with the LHC luminosity, are very low. Many of the input variables used in training the \BDTs and cuts are validated with both \Zee and \Zmumugamma data/\MC comparison plots. An example has been shown in a previous figure (see Fig.~\ref{fig:scale_smearing_analysis}) for the reconstructed dielectron mass for events passing the preselection described in Table~\ref{tab:preselection}.

As previously shown (in Sec.~\ref{sec:scale_smearing}) data/\MC comparisons of the \Zee decay are used to derive scale corrections for the data and smearing of the \MC simulation. Discrepancies between data and \MC in \Zee decays of important analysis variables are accounted for by introducing systematic uncertainties to cover them. In addition the ``tag and probe" method ~\cite{tag_and_probe} is used on \Zee decays to evaluate the signal efficiency for the preselection and analysis cuts. Several stages of the analysis are validated in this way and where appropriate systematic uncertainties are included to account for any data/\MC differences. Although the numbers and uncertainties themselves are derived from \Zee samples (because of the much higher statistics), they are always cross-checked with the \Zmumugamma sample.
